{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1\n",
    "В первой части на базе предложенных заготовок на Python предлагается реализовать алгоритм Витерби для создания PoS-теггера.   \n",
    "\n",
    "Заготовки и корпус для тестирования можно найти в архиве LabWork1.zip  \n",
    "В архиве можно найти pdf файл lm-pos.pdf, который содержит краткую справку по NLTK и описания лабораторной работы.  \n",
    "В рамках лабораторной работы требуется выполнить Assignment Part B - Part-of-Speech Tagging, пункты 1-5 (реализация пункта 6 не требуется).  \n",
    "\n",
    "pos.py используется для оценки точности.  \n",
    "solutionB.py - скелет приложения (требуется реализация ряда функций, подробности в lm-pos.pdf).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import collections \n",
    "from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "from tqdm import tqdm\n",
    "\n",
    "START_SYMBOL = '*'\n",
    "STOP_SYMBOL = 'STOP'\n",
    "RARE_SYMBOL = '_RARE_'\n",
    "RARE_WORD_MAX_FREQ = 5\n",
    "LOG_PROB_OF_ZERO = -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receives a list of tagged sentences and processes each sentence to generate a list of words and a list of tags.\n",
    "# Each sentence is a string of space separated \"WORD/TAG\" tokens, with a newline character in the end.\n",
    "# Remember to include start and stop symbols in yout returned lists, as defined by the constants START_SYMBOL and STOP_SYMBOL.\n",
    "# brown_words (the list of words) should be a list where every element is a list of the tags of a particular sentence.\n",
    "# brown_tags (the list of tags) should be a list where every element is a list of the tags of a particular sentence.\n",
    "def split_wordtags(brown_train):\n",
    "    brown_words = []\n",
    "    brown_tags = []\n",
    "    for line in brown_train:\n",
    "        tags = []\n",
    "        words = []\n",
    "        tokens = line.split()\n",
    "        for token in tokens:\n",
    "            word = token.rsplit('/', 1)\n",
    "            for i in range(0, len(word)-1):\n",
    "                words.append(word[i])\n",
    "            tags += [word[len(word)-1]]\n",
    "        brown_words.append(words)\n",
    "        brown_tags.append(tags)\n",
    "    return brown_words, brown_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes tags from the training data and calculates tag trigram probabilities.\n",
    "# It returns a python dictionary where the keys are tuples that represent the tag trigram, and the values are the log probability of that trigram\n",
    "def calc_trigrams(brown_tags):\n",
    "    tokens = []\n",
    "    c_bigram_values = collections.defaultdict(float)\n",
    "    c_trigram_values = collections.defaultdict(float)\n",
    "    q_values = collections.defaultdict(float)\n",
    "    for line in brown_tags:\n",
    "        tokens = ['*'] + line + ['STOP']\n",
    "        bigram_tuples = (tuple(nltk.bigrams(tokens)))\n",
    "        tokens = ['*'] + tokens\n",
    "        trigram_tuples = (tuple(nltk.trigrams(tokens)))\n",
    "        for pair in bigram_tuples:\n",
    "            c_bigram_values[pair] += 1.0\n",
    "        for triple in trigram_tuples:\n",
    "            c_trigram_values[triple] += 1.0\n",
    "        for key in c_trigram_values:\n",
    "            if key[0] == '*' and key[1] == '*':\n",
    "                q_values[key] = math.log(c_trigram_values[key],2).real - math.log(len(brown_tags),2).real\n",
    "            else:\n",
    "                q_values[key] = math.log(c_trigram_values[key],2).real - math.log(c_bigram_values[(key[0],key[1])],2).real \n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes output from calc_trigrams() and outputs it in the proper format\n",
    "def q2_output(q_values, filename):\n",
    "    outfile = open(filename, \"w\")\n",
    "    trigrams = q_values.keys()\n",
    "\n",
    "    # trigrams.sort()\n",
    "    trigrams = sorted(trigrams)\n",
    "    for trigram in trigrams:\n",
    "        output = \" \".join(['TRIGRAM', trigram[0], trigram[1], trigram[2], str(q_values[trigram])])\n",
    "        outfile.write(output + '\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the words from the training data and returns a set of all of the words that occur more than 5 times (use RARE_WORD_MAX_FREQ)\n",
    "# brown_words is a python list where every element is a python list of the words of a particular sentence.\n",
    "# Note: words that appear exactly 5 times should be considered rare!\n",
    "def calc_known(brown_words):\n",
    "    known_words = []\n",
    "    words = collections.defaultdict(float)\n",
    "    for line in brown_words:\n",
    "        for item in line:\n",
    "            words[item] += 1\n",
    "    for item in words:\n",
    "        if words[item] > 5:\n",
    "            known_words.append(item)\n",
    "    known_words = set(known_words)\n",
    "    return known_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the words from the training data and a set of words that should not be replaced for '_RARE_'\n",
    "# Returns the equivalent to brown_words but replacing the unknown words by '_RARE_' (use RARE_SYMBOL constant)\n",
    "def replace_rare(brown_words, known_words):\n",
    "    brown_words_rare = []\n",
    "    for line in brown_words:\n",
    "        sentence = []\n",
    "        for word in line:\n",
    "            if word not in known_words:\n",
    "                sentence.append('_RARE_')\n",
    "            else:\n",
    "                sentence.append(word)\n",
    "        sentence.append('STOP')\n",
    "        brown_words_rare.append(sentence)\n",
    "    return brown_words_rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the ouput from replace_rare and outputs it to a file\n",
    "def q3_output(rare, filename):\n",
    "    outfile = open(filename, 'w')\n",
    "    for sentence in rare:\n",
    "        outfile.write(' '.join(sentence[2:-1]) + '\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates emission probabilities and creates a set of all possible tags\n",
    "# The first return value is a python dictionary where each key is a tuple in which the first element is a word\n",
    "# and the second is a tag, and the value is the log probability of the emission of the word given the tag\n",
    "# The second return value is a set of all possible tags for this data set\n",
    "def calc_emission(brown_words_rare, brown_tags):\n",
    "    e_values = {}\n",
    "    taglist = []\n",
    "    zipped_nested_list = [zip(brown_words_rare[i], brown_tags[i]) for i in range(0, len(brown_words_rare))]\n",
    "    zipped_list = [j for i in zipped_nested_list for j in i]\n",
    "    d_zipped_list = collections.Counter(zipped_list)             # d->dictionary\n",
    "\n",
    "    temp_brown_words_rare = brown_words_rare[:]\n",
    "    temp_brown_words_rare = [j for i in temp_brown_words_rare for j in i]\n",
    "    d_temp_brown_words_rare = collections.Counter(temp_brown_words_rare)\n",
    "\n",
    "    temp_brown_tags = brown_tags[:]\n",
    "    temp_brown_tags = [j for i in temp_brown_tags for j in i]\n",
    "    d_temp_brown_tags = collections.Counter(temp_brown_tags)\n",
    "\n",
    "    for i in d_zipped_list:\n",
    "        e_values[i] = math.log(d_zipped_list[i]/(1.0*d_temp_brown_tags[i[1]]), 2)\n",
    "\n",
    "    for i in d_temp_brown_tags:\n",
    "        taglist.append(i)\n",
    "    taglist = set(taglist)\n",
    "    return e_values, taglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the output from calc_emissions() and outputs it\n",
    "def q4_output(e_values, filename):\n",
    "    outfile = open(filename, \"w\")\n",
    "    emissions = e_values.keys()\n",
    "    # emissions.sort()  for python 2\n",
    "    emissions = sorted(emissions)  \n",
    "    for item in emissions:\n",
    "        output = \" \".join([item[0], item[1], str(e_values[item])])\n",
    "        outfile.write(output + '\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes data to tag (brown_dev_words), a set of all possible tags (taglist), a set of all known words (known_words),\n",
    "# trigram probabilities (q_values) and emission probabilities (e_values) and outputs a list where every element is a tagged sentence \n",
    "# (in the WORD/TAG format, separated by spaces and with a newline in the end, just like our input tagged data)\n",
    "# brown_dev_words is a python list where every element is a python list of the words of a particular sentence.\n",
    "# taglist is a set of all possible tags\n",
    "# known_words is a set of all known words\n",
    "# q_values is from the return of calc_trigrams()\n",
    "# e_values is from the return of calc_emissions()\n",
    "# The return value is a list of tagged sentences in the format \"WORD/TAG\", separated by spaces. Each sentence is a string with a \n",
    "# terminal newline, not a list of tokens. Remember also that the output should not contain the \"_RARE_\" symbol, but rather the\n",
    "# original words of the sentence!\n",
    "def viterbi(brown_dev_words, taglist, known_words, q_values, e_values):\n",
    "    tagged = []\n",
    "    temp_brown_dev_words = brown_dev_words[:]\n",
    "\n",
    "    for sentence in temp_brown_dev_words:\n",
    "        temp = []\n",
    "        t = [START_SYMBOL, START_SYMBOL, 0]  # make it tuple, like tuple(t) and use it as key in q_values\n",
    "        for words in sentence:\n",
    "            d_word_tag_values = {}\n",
    "            if (words in known_words):\n",
    "                for key in e_values:\n",
    "                    if (key[0] == words):\n",
    "                        t[-1] = key[-1]\n",
    "                        if (q_values.get(tuple(t), LOG_PROB_OF_ZERO) != LOG_PROB_OF_ZERO):\n",
    "                            d_word_tag_values[(key, tuple(t))] = e_values[key] + q_values[tuple(t)]\n",
    "                        else:\n",
    "                            d_word_tag_values[(key, None)] = e_values[key] + LOG_PROB_OF_ZERO\n",
    "            else:\n",
    "                for key in e_values:\n",
    "                    if (key[0] == RARE_SYMBOL):\n",
    "                        t[-1] = key[-1]\n",
    "                        if (q_values.get(tuple(t), LOG_PROB_OF_ZERO) != LOG_PROB_OF_ZERO):\n",
    "                            d_word_tag_values[(key, tuple(t))] = e_values[key] + q_values[tuple(t)]\n",
    "                        else:\n",
    "                            d_word_tag_values[(key, None)] = e_values[key] + LOG_PROB_OF_ZERO\n",
    "\n",
    "            key_tuple_state = max(d_word_tag_values, key=d_word_tag_values.get)\n",
    "            t[-1] = key_tuple_state[0][-1]\n",
    "            temp.append(words + \"/\" + t[-1])\n",
    "            t.append(0)\n",
    "            t = t[1:]\n",
    "        tagged.append(\" \".join(temp[:]) + '\\n')\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the output of viterbi() and outputs it to file\n",
    "def q5_output(tagged, filename):\n",
    "    outfile = open(filename, 'w')\n",
    "    for sentence in tagged:\n",
    "        outfile.write(sentence)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part B time:  299.305008  sec\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'LabWork1/data/Brown/'\n",
    "OUTPUT_PATH = 'LabWork1/output/Brown/'\n",
    "\n",
    "def main():\n",
    "    # start timer\n",
    "    time.clock()\n",
    "\n",
    "    # open Brown training data\n",
    "    infile = open(DATA_PATH + \"Brown_tagged_train.txt\", \"r\")\n",
    "    brown_train = infile.readlines()\n",
    "    infile.close()\n",
    "\n",
    "    # split words and tags, and add start and stop symbols (question 1)\n",
    "    brown_words, brown_tags = split_wordtags(brown_train)\n",
    "\n",
    "    # calculate tag trigram probabilities (question 2)\n",
    "    q_values = calc_trigrams(brown_tags)\n",
    "\n",
    "    # question 2 output\n",
    "    q2_output(q_values, OUTPUT_PATH + 'B2.txt')\n",
    "\n",
    "    # calculate list of words with count > 5 (question 3)\n",
    "    known_words = calc_known(brown_words)\n",
    "\n",
    "    # get a version of brown_words with rare words replace with '_RARE_' (question 3)\n",
    "    brown_words_rare = replace_rare(brown_words, known_words)\n",
    "\n",
    "    # question 3 output\n",
    "    q3_output(brown_words_rare, OUTPUT_PATH + \"B3.txt\")\n",
    "\n",
    "    # calculate emission probabilities (question 4)\n",
    "    e_values, taglist = calc_emission(brown_words_rare, brown_tags)\n",
    "\n",
    "    # question 4 output\n",
    "    q4_output(e_values, OUTPUT_PATH + \"B4.txt\")\n",
    "\n",
    "    # delete unneceessary data\n",
    "    del brown_train\n",
    "    del brown_words_rare\n",
    "\n",
    "    # open Brown development data (question 5)\n",
    "    infile = open(DATA_PATH + \"Brown_dev.txt\", \"r\")\n",
    "    brown_dev = infile.readlines()\n",
    "    infile.close()\n",
    "\n",
    "    # format Brown development data here\n",
    "    brown_dev_words = []\n",
    "    for sentence in brown_dev:\n",
    "        brown_dev_words.append(sentence.split(\" \")[:-1])\n",
    "\n",
    "    # do viterbi on brown_dev_words (question 5)\n",
    "    viterbi_tagged = viterbi(brown_dev_words, taglist, known_words, q_values, e_values)\n",
    "\n",
    "    # question 5 output\n",
    "    q5_output(viterbi_tagged, OUTPUT_PATH + 'B5.txt')\n",
    "\n",
    "    # print total time to run Part B\n",
    "    print(\"Part B time: \", str(time.clock()),' sec')\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check (outputfile, referencefile):\n",
    "\n",
    "    infile = open(outputfile, \"r\")\n",
    "    user_sentences = infile.readlines()\n",
    "    infile.close()\n",
    "\n",
    "    infile = open(referencefile, \"r\")\n",
    "    correct_sentences = infile.readlines()\n",
    "    infile.close()\n",
    "\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for user_sent, correct_sent in zip(user_sentences, correct_sentences):\n",
    "        user_tok = user_sent.split()\n",
    "        correct_tok = correct_sent.split()\n",
    "\n",
    "        if len(user_tok) != len(correct_tok):\n",
    "            continue\n",
    "\n",
    "        for u, c in zip(user_tok, correct_tok):\n",
    "            if u == c:\n",
    "                num_correct += 1\n",
    "            total += 1\n",
    "\n",
    "    score = float(num_correct) / total * 100\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.68487501710086\n"
     ]
    }
   ],
   "source": [
    "print(check(OUTPUT_PATH+\"B5.txt\", DATA_PATH+\"Brown_tagged_dev.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2\n",
    "Вторая часть лабораторной работы связана с использованием созданного PoS-теггера и его использовании для стороннего корпуса.  \n",
    "Следует использовать корпус Universal Dependencies: http://universaldependencies.org/  \n",
    "\n",
    "Скачать Universal Dependency можно отсюда: https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1983\n",
    "\n",
    "\n",
    "Корпуса представляются в формате CoNLL. Нужно прочесть документацию о том, что это за формат, преобразовать его к тому же виду, что используется в первой части лабораторной работы (слово/тег).  \n",
    "Использовать PoS-теггер и получить показатели точности работы теггера, основанного на HMM.  \n",
    "\n",
    "Языки: Korean, Latin, Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_in = {'korean': 'LabWork1/data/UD_Korean', \n",
    "             'latin': 'LabWork1/data/UD_Latin-ITTB',\n",
    "             'russian': 'LabWork1/data/UD_Russian-SynTagRus'}\n",
    "\n",
    "languages_out = {'korean': 'LabWork1/output/Korean', \n",
    "             'latin': 'LabWork1/output/Latin',\n",
    "             'russian': 'LabWork1/output/Russian'}\n",
    "\n",
    "\n",
    "# # open Brown training data\n",
    "# infile = open(DATA_PATH + \"ko-ud-train.conllu\", \"r\")\n",
    "# brown_train = infile.readlines()\n",
    "# infile.close()\n",
    "\n",
    "# # split words and tags, and add start and stop symbols (question 1)\n",
    "# brown_words, brown_tags = split_wordtags(brown_train)\n",
    "\n",
    "# # calculate tag trigram probabilities (question 2)\n",
    "# q_values = calc_trigrams(brown_tags)\n",
    "\n",
    "# # calculate list of words with count > 5 (question 3)\n",
    "# known_words = calc_known(brown_words)\n",
    "\n",
    "# # get a version of brown_words with rare words replace with '_RARE_' (question 3)\n",
    "# brown_words_rare = replace_rare(brown_words, known_words)\n",
    "\n",
    "# # calculate emission probabilities (question 4)\n",
    "# e_values, taglist = calc_emission(brown_words_rare, brown_tags)\n",
    "\n",
    "# # delete unneceessary data\n",
    "# del brown_train\n",
    "# del brown_words_rare\n",
    "\n",
    "# # open Brown development data (question 5)\n",
    "# infile = open(DATA_PATH + \"Brown_dev.txt\", \"r\")\n",
    "# brown_dev = infile.readlines()\n",
    "# infile.close()\n",
    "\n",
    "# # format Brown development data here\n",
    "# brown_dev_words = []\n",
    "# for sentence in brown_dev:\n",
    "#     brown_dev_words.append(sentence.split(\" \")[:-1])\n",
    "\n",
    "# # do viterbi on brown_dev_words (question 5)\n",
    "# viterbi_tagged = viterbi(brown_dev_words, taglist, known_words, q_values, e_values)\n",
    "\n",
    "# # question 5 output\n",
    "# q5_output(viterbi_tagged, OUTPUT_PATH + 'B5.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 3\n",
    "Третья часть лабораторной работы предполагает решение той же задачи, с использованием алгоритма условно случайных полей CRF.\n",
    "(Возможно, придется спроектировать атрибуты)  \n",
    "\n",
    "Результаты лабораторной работы:  \n",
    "Мне на почту выслать реализованный PoS-теггер на HMM и CRF (Source code)  \n",
    "Преобразованный тренировочный и тестовый корпуса из Universal Dependencies в формате PoS-теггера;  \n",
    "Прислать показатели точности, которых достиг PoS-теггер;  \n",
    "Сравнить их друг с другом.  \n",
    "\n",
    "Замечания:  \n",
    "Для CRF предлагается (но не ограничивается) использовать http://www.chokkan.org/software/crfsuite/ (для Python есть wrapper: https://python-crfsuite.readthedocs.io/en/latest/).  \n",
    "Для HMM ограничений нет.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes data to tag (brown_dev_words), a set of all possible tags (taglist), a set of all known words (known_words),\n",
    "# trigram probabilities (q_values) and emission probabilities (e_values) and outputs a list where every element is a tagged sentence \n",
    "# (in the WORD/TAG format, separated by spaces and with a newline in the end, just like our input tagged data)\n",
    "# brown_dev_words is a python list where every element is a python list of the words of a particular sentence.\n",
    "# taglist is a set of all possible tags\n",
    "# known_words is a set of all known words\n",
    "# q_values is from the return of calc_trigrams()\n",
    "# e_values is from the return of calc_emissions()\n",
    "# The return value is a list of tagged sentences in the format \"WORD/TAG\", separated by spaces. Each sentence is a string with a \n",
    "# terminal newline, not a list of tokens. Remember also that the output should not contain the \"_RARE_\" symbol, but rather the\n",
    "# original words of the sentence!\n",
    "def conditional_random_field(brown_words, brown_tags):\n",
    "    train_words_tags = []\n",
    "    crf = CRFTagger(verbose=True)\n",
    "    for i in tqdm(range(len(brown_words))):\n",
    "        tmp = []\n",
    "        for j in range(len(brown_words[i])):\n",
    "            tmp.append((brown_words[i][j], brown_tags[i][j]))\n",
    "            #tmp.append((unicode(brown_words[i][j].decode('utf-8')), unicode(brown_tags[i][j].decode('utf-8'))))\n",
    "        train_words_tags.append(tmp)\n",
    "    crf.train(train_words_tags, 'model.crf.tagger')\n",
    "    return crf, train_words_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27491/27491 [00:00<00:00, 107670.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 0\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 42023\n",
      "Seconds required: 0.333\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.000000\n",
      "c2: 1.000000\n",
      "num_memories: 6\n",
      "max_iterations: 2147483647\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "***** Iteration #1 *****\n",
      "Loss: 808618.785917\n",
      "Feature norm: 5.000000\n",
      "Error norm: 72813.733703\n",
      "Active features: 42023\n",
      "Line search trials: 2\n",
      "Line search step: 0.000036\n",
      "Seconds required for this iteration: 1.223\n",
      "\n",
      "***** Iteration #2 *****\n",
      "Loss: 553697.951593\n",
      "Feature norm: 11.348525\n",
      "Error norm: 49535.832385\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #3 *****\n",
      "Loss: 474113.622321\n",
      "Feature norm: 15.107980\n",
      "Error norm: 45528.832898\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.433\n",
      "\n",
      "***** Iteration #4 *****\n",
      "Loss: 424156.663681\n",
      "Feature norm: 16.555004\n",
      "Error norm: 26607.538253\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #5 *****\n",
      "Loss: 369428.374090\n",
      "Feature norm: 20.156717\n",
      "Error norm: 19933.496619\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.432\n",
      "\n",
      "***** Iteration #6 *****\n",
      "Loss: 327090.366411\n",
      "Feature norm: 24.622030\n",
      "Error norm: 19359.465546\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #7 *****\n",
      "Loss: 278786.256925\n",
      "Feature norm: 32.994379\n",
      "Error norm: 19555.826819\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #8 *****\n",
      "Loss: 247543.624334\n",
      "Feature norm: 39.736239\n",
      "Error norm: 13265.723032\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.433\n",
      "\n",
      "***** Iteration #9 *****\n",
      "Loss: 228731.743733\n",
      "Feature norm: 44.598575\n",
      "Error norm: 9132.183401\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #10 *****\n",
      "Loss: 211777.799788\n",
      "Feature norm: 49.816504\n",
      "Error norm: 7600.346129\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.436\n",
      "\n",
      "***** Iteration #11 *****\n",
      "Loss: 203937.646984\n",
      "Feature norm: 58.880515\n",
      "Error norm: 20774.529119\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #12 *****\n",
      "Loss: 188100.588618\n",
      "Feature norm: 59.579593\n",
      "Error norm: 7098.009633\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.435\n",
      "\n",
      "***** Iteration #13 *****\n",
      "Loss: 181599.470081\n",
      "Feature norm: 59.850416\n",
      "Error norm: 5853.987436\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #14 *****\n",
      "Loss: 172881.758365\n",
      "Feature norm: 60.665451\n",
      "Error norm: 7244.697020\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #15 *****\n",
      "Loss: 162413.451482\n",
      "Feature norm: 63.207491\n",
      "Error norm: 8883.134725\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #16 *****\n",
      "Loss: 155996.128380\n",
      "Feature norm: 66.468400\n",
      "Error norm: 7527.846464\n",
      "Active features: 42023\n",
      "Line search trials: 2\n",
      "Line search step: 0.469750\n",
      "Seconds required for this iteration: 0.866\n",
      "\n",
      "***** Iteration #17 *****\n",
      "Loss: 150162.534642\n",
      "Feature norm: 69.245986\n",
      "Error norm: 4673.707136\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.435\n",
      "\n",
      "***** Iteration #18 *****\n",
      "Loss: 144298.828124\n",
      "Feature norm: 72.968652\n",
      "Error norm: 4852.758619\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.433\n",
      "\n",
      "***** Iteration #19 *****\n",
      "Loss: 138221.568560\n",
      "Feature norm: 78.023763\n",
      "Error norm: 4967.103996\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #20 *****\n",
      "Loss: 132875.813345\n",
      "Feature norm: 84.867907\n",
      "Error norm: 5224.200941\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #21 *****\n",
      "Loss: 129179.705990\n",
      "Feature norm: 87.078504\n",
      "Error norm: 4121.734704\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.433\n",
      "\n",
      "***** Iteration #22 *****\n",
      "Loss: 125527.061858\n",
      "Feature norm: 89.761012\n",
      "Error norm: 3461.330757\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #23 *****\n",
      "Loss: 122799.864541\n",
      "Feature norm: 92.312249\n",
      "Error norm: 3453.833027\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #24 *****\n",
      "Loss: 120330.368352\n",
      "Feature norm: 96.266789\n",
      "Error norm: 5037.617660\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.433\n",
      "\n",
      "***** Iteration #25 *****\n",
      "Loss: 117798.400995\n",
      "Feature norm: 97.389153\n",
      "Error norm: 2772.880703\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.436\n",
      "\n",
      "***** Iteration #26 *****\n",
      "Loss: 115539.309876\n",
      "Feature norm: 98.399691\n",
      "Error norm: 3106.093776\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.433\n",
      "\n",
      "***** Iteration #27 *****\n",
      "Loss: 112770.487505\n",
      "Feature norm: 101.095788\n",
      "Error norm: 2839.451693\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #28 *****\n",
      "Loss: 110109.864693\n",
      "Feature norm: 106.692355\n",
      "Error norm: 4720.167954\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #29 *****\n",
      "Loss: 108211.715802\n",
      "Feature norm: 110.434539\n",
      "Error norm: 4181.465541\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.433\n",
      "\n",
      "***** Iteration #30 *****\n",
      "Loss: 107065.731808\n",
      "Feature norm: 108.838649\n",
      "Error norm: 2320.984620\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #31 *****\n",
      "Loss: 106344.670816\n",
      "Feature norm: 109.525082\n",
      "Error norm: 1709.449508\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.435\n",
      "\n",
      "***** Iteration #32 *****\n",
      "Loss: 104705.788240\n",
      "Feature norm: 112.420278\n",
      "Error norm: 1804.644302\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #33 *****\n",
      "Loss: 102984.610200\n",
      "Feature norm: 116.199633\n",
      "Error norm: 2190.476464\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.434\n",
      "\n",
      "***** Iteration #34 *****\n",
      "Loss: 102324.540402\n",
      "Feature norm: 122.509409\n",
      "Error norm: 5388.474626\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.435\n",
      "\n",
      "***** Iteration #35 *****\n",
      "Loss: 100418.411324\n",
      "Feature norm: 122.468710\n",
      "Error norm: 1695.093686\n",
      "Active features: 42023\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# start timer\n",
    "time.clock()\n",
    "\n",
    "# open Brown training data\n",
    "infile = open(DATA_PATH + \"Brown_tagged_train.txt\", \"r\")\n",
    "brown_train = infile.readlines()\n",
    "infile.close()\n",
    "\n",
    "# split words and tags, and add start and stop symbols (question 1)\n",
    "brown_words, brown_tags = split_wordtags(brown_train)\n",
    "\n",
    "# do viterbi on brown_dev_words (question 5)\n",
    "crf, crf_tagged = conditional_random_field(brown_words, brown_tags)\n",
    "\n",
    "# open Brown development data (question 5)\n",
    "infile = open(DATA_PATH + \"Brown_dev.txt\", \"r\")\n",
    "brown_dev = infile.readlines()\n",
    "infile.close()\n",
    "\n",
    "# format Brown development data here\n",
    "brown_dev_words = []\n",
    "for sentence in brown_dev:\n",
    "    brown_dev_words.append(sentence.split(\" \")[:-1])\n",
    "\n",
    "results = crf.tag_sents(brown_dev_words)\n",
    "formated_results = [\" \".join([word[0] + \"/\" + word[1] for word in sentence]) + \"\\n\" for sentence in results]    \n",
    "    \n",
    "# question 5 output\n",
    "q5_output(formated_results, OUTPUT_PATH + 'B5_CFG.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check(OUTPUT_PATH+\"B5_CFG.txt\", DATA_PATH+\"Brown_tagged_dev.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|         | HMM | CRF |\n",
    "|---------|---------|-----|\n",
    "| Brown   |   91.68 | 95.72  |\n",
    "| Korean  |         |     |\n",
    "| Latin   |         |     |\n",
    "| Russian |         |     |\n",
    "\n",
    "\n",
    "https://github.com/UniversalDependencies/UD_Korean  \n",
    "https://github.com/UniversalDependencies/UD_Latin-ITTB  \n",
    "https://github.com/UniversalDependencies/UD_Russian-SynTagRus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
